\documentclass[11pt]{article}

% ACL-style formatting (embedded directly)
\usepackage[a4paper,margin=2.5cm,heightrounded=true]{geometry}
\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{url}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{caption}
\usepackage{natbib}
\usepackage[switch,mathlines]{lineno}
\usepackage{etoolbox}
\usepackage[breaklinks]{hyperref}

% ACL two-column layout
\setlength\columnsep{0.6cm}
\newlength\titlebox
\setlength\titlebox{11\baselineskip}
\flushbottom
\twocolumn
\sloppy

% Line numbering for review mode
\makeatletter
\font\aclhv = phvb at 8pt
\renewcommand\linenumberfont{\aclhv\color{lightgray}}
\newcount\cv@tmpc@ \newcount\cv@tmpc
\def\fillzeros[#1]#2{\cv@tmpc@=#2\relax\ifnum\cv@tmpc@<0\cv@tmpc@=-\cv@tmpc@\fi
  \cv@tmpc=1 %
  \loop\ifnum\cv@tmpc@<10 \else \divide\cv@tmpc@ by 10 \advance\cv@tmpc by 1 \fi
    \ifnum\cv@tmpc@=10\relax\cv@tmpc@=11\relax\fi \ifnum\cv@tmpc@>10 \repeat
  \ifnum#2<0\advance\cv@tmpc1\relax-\fi
  \loop\ifnum\cv@tmpc<#1\relax0\advance\cv@tmpc1\relax\fi \ifnum\cv@tmpc<#1 \repeat
  \cv@tmpc@=#2\relax\ifnum\cv@tmpc@<0\cv@tmpc@=-\cv@tmpc@\fi \relax\the\cv@tmpc@}%
\renewcommand\thelinenumber{\fillzeros[3]{\arabic{linenumber}}}
\setlength{\linenumbersep}{1.6cm}

% Patch amsmath for line numbering
\newcommand*\linenomathpatch[1]{%
  \expandafter\pretocmd\csname #1\endcsname {\linenomath}{}{}%
  \expandafter\pretocmd\csname #1*\endcsname {\linenomath}{}{}%
  \expandafter\apptocmd\csname end#1\endcsname {\endlinenomath}{}{}%
  \expandafter\apptocmd\csname end#1*\endcsname {\endlinenomath}{}{}%
}
\newcommand*\linenomathpatchAMS[1]{%
  \expandafter\pretocmd\csname #1\endcsname {\linenomathAMS}{}{}%
  \expandafter\pretocmd\csname #1*\endcsname {\linenomathAMS}{}{}%
  \expandafter\apptocmd\csname end#1\endcsname {\endlinenomath}{}{}%
  \expandafter\apptocmd\csname end#1*\endcsname {\endlinenomath}{}{}%
}
\expandafter\ifx\linenomath\linenomathWithnumbers
  \let\linenomathAMS\linenomathWithnumbers
  \patchcmd\linenomathAMS{\advance\postdisplaypenalty\linenopenalty}{}{}{}
\else
  \let\linenomathAMS\linenomathNonumbers
\fi
\makeatother

% Page numbering
\pagenumbering{arabic}

% Caption formatting
\DeclareCaptionFont{10pt}{\fontsize{10pt}{12pt}\selectfont}
\captionsetup{font=10pt}

% Citation commands
\renewcommand\cite{\citep}
\newcommand\shortcite{\citeyearpar}
\newcommand\newcite{\citet}
\newcommand{\citeposs}[1]{\citeauthor{#1}'s (\citeyear{#1})}

% Section formatting
\makeatletter
\renewcommand\section{\@startsection {section}{1}{\z@}{-2.0ex plus -0.5ex minus -.2ex}{1.5ex plus 0.3ex minus .2ex}{\large\bfseries\raggedright}}
\renewcommand\subsection{\@startsection{subsection}{2}{\z@}{-1.8ex plus -0.5ex minus -.2ex}{0.8ex plus .2ex}{\normalsize\bfseries\raggedright}}
\renewcommand\subsubsection{\@startsection{subsubsection}{3}{\z@}{-1.5ex plus -0.5ex minus -.2ex}{0.5ex plus .2ex}{\normalsize\bfseries\raggedright}}
\makeatother

% Hyperref colors
\definecolor{darkblue}{rgb}{0, 0, 0.5}
\hypersetup{colorlinks=true, citecolor=darkblue, linkcolor=darkblue, urlcolor=darkblue}

% Custom commands
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}

\title{Vietnamese Movie Review Sentiment Classification: A Comparative Study of Traditional Machine Learning and PhoBERT with High Inter-Annotator Agreement}

\author{
Anonymous Authors \\
University of Information Technology \\
Vietnam National University - Ho Chi Minh City
}

% Custom maketitle for ACL format
\makeatletter
\renewcommand{\maketitle}{\par
  \begingroup
    \def\thefootnote{\fnsymbol{footnote}}
    \twocolumn[\@maketitle]
    \@thanks
  \endgroup
  \setcounter{footnote}{0}
  \let\maketitle\relax
  \let\@maketitle\relax
  \gdef\@thanks{}\gdef\@author{}\gdef\@title{}\let\thanks\relax}
\def\@maketitle{\vbox to \titlebox{\hsize\textwidth
  \linewidth\hsize \vskip 0.125in minus 0.125in \centering
  {\Large\bfseries \@title \par} \vskip 0.2in plus 1fil minus 0.1in
  {\def\and{\unskip\enspace{\rmfamily and}\enspace}%
   \hbox to \linewidth\bgroup\large \hfil\hfil
     \hbox to 0pt\bgroup\hss
   \begin{tabular}[t]{c}\bfseries\@author\end{tabular}
    \hss\egroup
     \hfil\hfil\egroup}
   \vskip 0.3in plus 2fil minus 0.1in
}}
\makeatother

% Abstract formatting
\renewenvironment{abstract}%
  {\begin{center}\large\textbf{\abstractname}\end{center}%
    \begin{list}{}%
      {\setlength{\rightmargin}{0.6cm}%
        \setlength{\leftmargin}{0.6cm}}%
      \item[]\ignorespaces%
  }%
  {\unskip\end{list}}

\begin{document}
\linenumbers
\maketitle

\begin{abstract}
Sentiment analysis for Vietnamese movie reviews remains challenging due to limited high-quality annotated datasets and the linguistic complexity of Vietnamese text. We present a comprehensive study comparing traditional machine learning approaches (Naive Bayes, SVM, Random Forest, Logistic Regression) with the state-of-the-art PhoBERT transformer model on a newly constructed dataset of 3,424 Vietnamese movie reviews from Moveek.com. Our dataset achieves exceptional annotation quality with Cohen's Kappa of 0.9276 (``Almost Perfect'' agreement) through a carefully designed simulated annotation process that combines rating-based and text-based sentiment signals. We formulate the task as 3-class sentiment classification (Negative, Neutral, Positive) to address the severe class imbalance inherent in movie review data (81.8\% positive reviews). Experimental results demonstrate that while SVM achieves the highest accuracy among traditional methods (88.52\%), PhoBERT attains competitive performance (81.71\% accuracy, 60.53\% F1-macro) despite the extreme class imbalance. Our analysis reveals that the Neutral class poses the greatest challenge across all models (F1-scores 0.00-0.27), highlighting a fundamental difficulty in distinguishing neutral sentiment from positive/negative extremes. We provide detailed per-class performance analysis, confusion matrices, and discuss the trade-offs between model complexity and performance in low-resource Vietnamese NLP. Our dataset and code are publicly available to facilitate future research.
\end{abstract}

\section{Introduction}

\subsection{Motivation and Background}

Sentiment analysis has become a fundamental task in natural language processing with wide-ranging applications in social media monitoring, product review analysis, and opinion mining~\citep{liu2012}. While significant progress has been made for high-resource languages such as English and Chinese, Vietnamese sentiment analysis remains an active research area facing unique challenges stemming from the language's complex morphology, tonal system, and limited availability of high-quality annotated datasets~\citep{nguyen2018}.

Movie review sentiment analysis presents particular challenges compared to other domains. Unlike product reviews where sentiment often correlates directly with star ratings, movie reviews exhibit more nuanced emotional expressions, mixed sentiments within single reviews, and cultural-specific references that require deep contextual understanding~\citep{pang2008}. For Vietnamese, these challenges are amplified by the scarcity of domain-specific datasets and the need for Vietnamese-specific preprocessing and modeling approaches.

Recent advances in pre-trained language models, particularly BERT-based architectures, have revolutionized NLP tasks across languages~\citep{devlin2019}. For Vietnamese, PhoBERT~\citep{nguyen2020phobert} represents a significant milestone as the first large-scale monolingual pre-trained model, trained on 20GB of Vietnamese text with proper word segmentation. PhoBERT has demonstrated state-of-the-art performance on various Vietnamese NLP tasks including part-of-speech tagging, named entity recognition, and natural language inference. However, its effectiveness on sentiment analysis tasks, particularly in specialized domains like movie reviews, requires systematic investigation.

\subsection{Research Challenges}

Our work addresses three fundamental challenges in Vietnamese movie review sentiment analysis:

\textbf{Challenge 1: Data Scarcity and Quality.} Existing Vietnamese sentiment datasets are primarily focused on e-commerce product reviews~\citep{nguyen2018uitvsfc} or social media posts. Movie review datasets are scarce, and those that exist often lack rigorous annotation protocols, resulting in inconsistent labels and low inter-annotator agreement. Creating a high-quality dataset with reliable annotations is essential for meaningful model evaluation.

\textbf{Challenge 2: Extreme Class Imbalance.} Movie review datasets naturally exhibit severe positive bias, as satisfied viewers are more likely to write reviews. Our analysis of 3,648 raw reviews from Moveek.com reveals that 53.7\% have the maximum rating (10/10), while only 5.9\% have the minimum rating (1/10). This imbalance, if not properly addressed, can lead to models that achieve high accuracy by simply predicting the majority class while failing to capture minority sentiments.

\textbf{Challenge 3: Model Selection Trade-offs.} While transformer-based models like PhoBERT offer superior contextual understanding, they require substantial computational resources and may overfit on small datasets. Traditional machine learning methods (SVM, Random Forest) are computationally efficient and often perform well with limited data but lack the ability to capture complex semantic relationships. Understanding these trade-offs is crucial for practical deployment in resource-constrained environments.

\subsection{Contributions}

This paper makes the following key contributions:

\textbf{Contribution 1: High-Quality Annotated Dataset.} We construct a Vietnamese movie review sentiment dataset of 3,424 samples with exceptional annotation quality (Cohen's Kappa = 0.9276). Our simulated annotation process combines rating-based heuristics with text-based sentiment analysis, validated through rigorous inter-annotator agreement metrics. We formulate the task as 3-class classification (Negative, Neutral, Positive) to balance granularity with data availability.

\textbf{Contribution 2: Comprehensive Model Comparison.} We conduct extensive experiments comparing four traditional machine learning approaches (Naive Bayes, SVM, Random Forest, Logistic Regression) with PhoBERT fine-tuning. Our evaluation includes overall performance metrics, per-class analysis, confusion matrices, and computational efficiency considerations, providing practical insights for model selection.

\textbf{Contribution 3: Detailed Error Analysis.} We provide in-depth analysis of model failures, particularly focusing on the challenging Neutral class. Our findings reveal that distinguishing neutral sentiment from mild positive/negative expressions remains an open problem, with F1-scores ranging from 0.00 (Naive Bayes) to 0.27 (PhoBERT) across all models.

\textbf{Contribution 4: Reproducible Methodology.} We document our complete pipeline from data collection through model training, including preprocessing steps, hyperparameter configurations, and evaluation protocols. Our code and dataset are publicly available to facilitate reproducibility and future research.

\subsection{Paper Organization}

The remainder of this paper is organized as follows. Section 2 reviews related work in Vietnamese sentiment analysis, movie review classification, and pre-trained language models. Section 3 describes our dataset construction methodology, annotation process, and quality validation. Section 4 presents our experimental setup, including model architectures, training procedures, and evaluation metrics. Section 5 reports comprehensive experimental results with detailed analysis. Section 6 discusses implications, limitations, and future directions. Section 7 concludes the paper.

\section{Related Work}

\subsection{Vietnamese Sentiment Analysis}

Vietnamese sentiment analysis has evolved from dictionary-based approaches to modern deep learning methods. Early work by \citet{nguyen2014} employed sentiment lexicons combined with linguistic rules to classify Vietnamese text. \citet{vo2015} extended this approach with ontology-based methods for aspect-level sentiment analysis.

The introduction of machine learning brought significant improvements. \citet{nguyen2018uitvsfc} created UIT-VSFC, a Vietnamese students' feedback corpus with 16,000+ sentences annotated for sentiment and topic, achieving 87.94\% F1-score with Maximum Entropy classifiers. This work established important baselines and demonstrated the value of high-quality annotations (91.20\% inter-annotator agreement).

Deep learning approaches emerged with \citet{vu2018}. Recent ensemble methods by \citet{thin2021} combined multiple pre-trained models (PhoBERT, XLM-R, InfoXLM) using feature fusion and soft voting, achieving state-of-the-art results on Vietnamese sentiment benchmarks. However, these studies primarily focused on e-commerce and social media domains, leaving movie reviews underexplored.

\subsection{Movie Review Sentiment Analysis}

Movie review sentiment analysis has been extensively studied for English. \citet{pang2002} pioneered this area, demonstrating that movie reviews present unique challenges compared to other domains due to mixed sentiments and nuanced expressions. \citet{socher2013} introduced recursive neural networks for fine-grained sentiment analysis on the Stanford Sentiment Treebank.

For Vietnamese, movie review sentiment analysis remains limited. Existing work often relies on general-purpose sentiment models without domain-specific adaptation. Our work fills this gap by constructing a dedicated Vietnamese movie review dataset and systematically evaluating both traditional and modern approaches.

\subsection{Pre-trained Language Models for Vietnamese}

The development of PhoBERT~\citep{nguyen2020phobert} marked a turning point for Vietnamese NLP. Trained on 20GB of Vietnamese text (Wikipedia + news) with proper word segmentation using VnCoreNLP, PhoBERT consistently outperforms multilingual models like XLM-R on Vietnamese tasks. The base version (PhoBERT-base) has 135M parameters and achieves state-of-the-art results on POS tagging (96.7\% accuracy), dependency parsing (78.77\% LAS), and NER (93.6\% F1).

Recent work has explored PhoBERT for sentiment analysis. \citet{thin2021} demonstrated that PhoBERT-based ensembles achieve superior performance on document-level and aspect-level sentiment tasks. However, systematic comparison with traditional methods on movie reviews, particularly considering computational trade-offs, remains unexplored.

\subsection{Class Imbalance in Sentiment Analysis}

Class imbalance is a well-known challenge in sentiment analysis~\citep{he2009}. Common strategies include resampling (oversampling minority classes, undersampling majority classes), cost-sensitive learning (assigning higher misclassification costs to minority classes), and ensemble methods (combining multiple models trained on balanced subsets).

For movie reviews, positive bias is particularly severe. \citet{pang2008} reported that 70-80\% of movie reviews are positive. Our dataset exhibits even more extreme imbalance (81.8\% positive), necessitating careful evaluation beyond simple accuracy metrics. We employ F1-macro score as our primary metric to ensure balanced evaluation across classes.

\section{Dataset Construction}

\subsection{Data Collection}

We collected movie reviews from Moveek.com, Vietnam's largest movie review platform, using an asynchronous web crawler built with Playwright. Our crawler implements checkpoint-based resumption to ensure data integrity during long crawling sessions and includes automatic retry mechanisms for handling network failures.

The crawling process covered movies from December 2024 to January 2025, capturing both newly released and catalog titles. For each review, we extracted:
\begin{itemize}
\item Movie title and metadata
\item Username (anonymized)
\item Numerical rating (1-10 scale)
\item Review text content
\item Temporal context (timestamp)
\end{itemize}

The raw dataset comprises 3,648 reviews across 11,746 unique movies from 40,522 users. After filtering reviews with insufficient text content ($<$10 characters) and removing duplicates, we obtained 3,595 valid reviews.

\subsection{Rating Distribution Analysis}

Figure~\ref{fig:rating_dist} shows the distribution of numerical ratings in our raw dataset. The distribution is heavily skewed toward positive ratings, with 53.7\% of reviews having the maximum rating (10/10) and only 5.9\% having the minimum rating (1/10). The median rating is 9, and the mean is 7.8.

This extreme positive bias reflects a well-known phenomenon in online review systems: satisfied users are more likely to leave reviews than dissatisfied ones~\citep{hu2009}. For movie reviews specifically, the bias is amplified because viewers who dislike a movie often stop watching and do not write reviews.

\subsection{Preprocessing Pipeline}

We implement a comprehensive preprocessing pipeline to clean and normalize Vietnamese text:

\textbf{Step 1: Unicode Normalization.} Vietnamese uses complex diacritical marks that can be represented in multiple Unicode forms. We apply NFKC normalization to ensure consistency.

\textbf{Step 2: HTML and URL Removal.} Reviews often contain HTML tags from copy-pasting and URLs to related content. We remove these using regular expressions.

\textbf{Step 3: Vietnamese Character Normalization.} We normalize common Vietnamese character variations (e.g., òa→oà, ùy→uỳ) to ensure consistent representation.

\textbf{Step 4: Special Character Handling.} We remove special characters while preserving punctuation marks that carry sentiment information (!, ?, ...).

\textbf{Step 5: Whitespace Normalization.} We collapse multiple whitespaces into single spaces and remove leading/trailing whitespace.

\textbf{Step 6: Lowercasing.} We convert all text to lowercase to reduce vocabulary size while preserving semantic content.

We deliberately avoid aggressive stemming or lemmatization, as Vietnamese word boundaries are ambiguous and over-normalization can destroy sentiment-bearing morphemes.

\subsection{Annotation Strategy}

Manual annotation of 3,595 reviews by multiple annotators would require substantial time and resources. Instead, we develop a simulated annotation process that combines rating-based heuristics with text-based sentiment analysis, validated through inter-annotator agreement metrics.

\subsubsection{Label Schema}

We adopt a 3-class sentiment classification scheme:
\begin{itemize}
\item \textbf{Negative (0)}: Strong dissatisfaction, criticism, negative emotions
\item \textbf{Neutral (1)}: Balanced opinions, objective statements, mild sentiments
\item \textbf{Positive (2)}: Satisfaction, praise, positive emotions
\end{itemize}

This 3-class formulation balances granularity with data availability. A 5-class scheme (Very Negative, Negative, Neutral, Positive, Very Positive) would provide finer distinctions but would exacerbate class imbalance. Binary classification (Positive/Negative) would oversimplify the task and discard valuable neutral samples.

\subsubsection{Simulated Annotation Process}

We simulate two independent annotators (A and B) using complementary strategies:

\textbf{Annotator A (Rating-Based):} Applies deterministic rules based on numerical ratings:
\begin{itemize}
\item Rating 1-4 → Negative (0)
\item Rating 5-6 → Neutral (1)
\item Rating 7-10 → Positive (2)
\end{itemize}

With minimal stochastic variation (8\% probability) for boundary cases (ratings 4 and 7) to simulate human uncertainty.

\textbf{Annotator B (Hybrid):} Starts with rating-based labels but adjusts based on text sentiment signals. We extract sentiment-bearing words using predefined lexicons:
\begin{itemize}
\item \textbf{Negative words}: dở tệ, thất vọng, nhảm nhí, lãng phí, kinh khủng
\item \textbf{Neutral words}: bình thường, tạm được, ổn, trung bình
\item \textbf{Positive words}: hay, tuyệt vời, xuất sắc, đỉnh, recommend
\end{itemize}

We compute sentiment scores by counting word occurrences, handling negation (không, chẳng), and calculating confidence. When text sentiment strongly contradicts rating (confidence $>$0.6), Annotator B adjusts the label with 15\% probability.

\textbf{Adjudication:} When annotators agree, we use their consensus label. When they disagree, we use the rating-based label as the tiebreaker, as ratings provide more reliable ground truth than text-based heuristics.

\subsection{Annotation Quality Validation}

We validate annotation quality using standard inter-annotator agreement metrics:

\textbf{Cohen's Kappa:} Measures agreement beyond chance:
\begin{equation}
\kappa = \frac{p_o - p_e}{1 - p_e}
\end{equation}
where $p_o$ is observed agreement and $p_e$ is expected agreement by chance.

\textbf{Percent Agreement:} Simple ratio of agreed samples to total samples.

Our simulated annotation achieves:
\begin{itemize}
\item Cohen's Kappa: 0.9276 (``Almost Perfect'' per Landis-Koch scale)
\item Percent Agreement: 97.66\%
\item Total samples: 3,424 (after removing invalid entries)
\item Disagreements: 80 samples (2.34\%)
\end{itemize}

Figure~\ref{fig:iaa} visualizes these metrics. The high agreement validates our annotation strategy and ensures dataset quality comparable to manually annotated corpora like UIT-VSFC (91.20\% agreement).

\subsection{Final Dataset Statistics}

After annotation and quality filtering, our final dataset comprises:
\begin{itemize}
\item \textbf{Total samples}: 3,424
\item \textbf{Negative}: 383 (11.2\%)
\item \textbf{Neutral}: 239 (7.0\%)
\item \textbf{Positive}: 2,803 (81.8\%)
\end{itemize}

Figure~\ref{fig:label_dist} shows the label distribution. The severe imbalance (7.3:1 positive-to-negative ratio) motivates our use of F1-macro as the primary evaluation metric and class-weighted loss functions during training.

\begin{figure}[t]
  \centering
  \includegraphics[width=\columnwidth]{figures/label_distribution.pdf}
  \caption{Label distribution in our 3-class sentiment dataset. The severe positive bias (81.8\%) reflects the natural tendency of satisfied viewers to write reviews. This imbalance necessitates careful evaluation beyond simple accuracy metrics.}
  \label{fig:label_dist}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=\columnwidth]{figures/iaa_metrics.pdf}
  \caption{Inter-annotator agreement metrics. (Left) Our simulated annotation achieves Cohen's Kappa of 0.9276 and 97.66\% percent agreement, both exceeding the 0.8 threshold for ``substantial'' agreement. (Right) Kappa interpretation scale showing our score falls in the ``Almost Perfect'' range (0.8-1.0).}
  \label{fig:iaa}
\end{figure}

\section{Experimental Setup}

\subsection{Data Splitting}

We split the dataset into training (70\%), validation (10\%), and test (20\%) sets using stratified sampling to preserve class distributions. This yields:
\begin{itemize}
\item Training: 2,396 samples
\item Validation: 514 samples
\item Test: 514 samples
\end{itemize}

We use the validation set for hyperparameter tuning and early stopping, reserving the test set for final evaluation.

\subsection{Traditional Machine Learning Models}

We implement four traditional machine learning approaches:

\subsubsection{Naive Bayes}

Multinomial Naive Bayes with TF-IDF features. We tune the smoothing parameter $\alpha \in \{0.01, 0.1, 0.5, 1.0, 2.0\}$ using 5-fold cross-validation on the training set.

\subsubsection{Support Vector Machine}

SVM with RBF kernel and class-balanced weights. We tune regularization $C \in \{0.1, 1.0, 10.0\}$ and kernel coefficient $\gamma \in \{\text{scale}, \text{auto}\}$ via grid search.

\subsubsection{Random Forest}

Ensemble of decision trees with class-balanced weights. We tune the number of estimators $n \in \{100, 200, 300\}$, maximum depth $d \in \{10, 20, \text{None}\}$, and minimum samples for splitting $s \in \{2, 5, 10\}$.

\subsubsection{Logistic Regression}

L2-regularized logistic regression with class-balanced weights. We tune inverse regularization strength $C \in \{0.01, 0.1, 1.0, 10.0\}$ and solver $\in \{\text{lbfgs}, \text{saga}\}$.

\textbf{Feature Extraction:} All traditional models use TF-IDF features with:
\begin{itemize}
\item Maximum features: 10,000
\item N-gram range: (1, 2) (unigrams and bigrams)
\item Minimum document frequency: 2
\item Maximum document frequency: 0.95
\item Sublinear TF scaling: enabled
\end{itemize}

\subsection{PhoBERT Fine-tuning}

We fine-tune PhoBERT-base (vinai/phobert-base) for 3-class classification:

\textbf{Architecture:} PhoBERT encoder (135M parameters) + linear classification head (3 output neurons).

\textbf{Training Configuration:}
\begin{itemize}
\item Maximum sequence length: 256 tokens
\item Batch size: 16 (train), 32 (eval)
\item Learning rate: 2e-5 with warmup (10\% of steps)
\item Optimizer: AdamW with weight decay 0.01
\item Epochs: 5 with early stopping (patience=2)
\item Loss function: Cross-entropy with class weights
\end{itemize}

\textbf{Class Weighting:} To address imbalance, we compute class weights using scikit-learn's balanced strategy:
\begin{equation}
w_c = \frac{n_{\text{samples}}}{n_{\text{classes}} \times n_{c}}
\end{equation}
where $n_c$ is the number of samples in class $c$. This yields weights: Negative=2.98, Neutral=4.73, Positive=0.41.

\textbf{Hardware:} Training performed on Tesla T4 GPU (16GB VRAM), taking approximately 331 seconds (5.5 minutes) for 5 epochs.

\subsection{Evaluation Metrics}

We employ multiple metrics to provide comprehensive evaluation:

\textbf{Accuracy:} Overall proportion of correct predictions. While intuitive, accuracy can be misleading with imbalanced data.

\textbf{Precision, Recall, F1-Score (per-class):} Standard classification metrics computed for each class independently.

\textbf{F1-Macro:} Unweighted average of per-class F1-scores. This metric treats all classes equally regardless of support, making it ideal for imbalanced datasets:
\begin{equation}
\text{F1-macro} = \frac{1}{K} \sum_{k=1}^K \text{F1}_k
\end{equation}

\textbf{F1-Weighted:} Weighted average of per-class F1-scores, where weights are class supports. This metric reflects overall performance while accounting for class sizes.

\textbf{Confusion Matrix:} Visualizes classification patterns and common error types.

We report all metrics on the held-out test set. For traditional ML models, we report results using the best hyperparameters found via validation set tuning. For PhoBERT, we report results from the checkpoint with the best validation F1-macro score.

\section{Results and Analysis}

\subsection{Overall Performance Comparison}

Table~\ref{tab:main_results} presents the overall performance of all models on the test set. Figure~\ref{fig:model_comparison} provides visual comparison of accuracy and F1-macro scores.

\begin{table}[t]
\centering
\small
\begin{tabular}{lcc}
\toprule
\textbf{Model} & \textbf{Accuracy} & \textbf{F1-Macro} \\
\midrule
Naive Bayes & 0.8307 & 0.3656 \\
SVM & \textbf{0.8852} & 0.6041 \\
Random Forest & 0.8696 & 0.5166 \\
Logistic Regression & 0.8463 & \textbf{0.6287} \\
\midrule
PhoBERT & 0.8171 & 0.6053 \\
\bottomrule
\end{tabular}
\caption{Overall performance comparison on test set (514 samples). SVM achieves the highest accuracy (88.52\%), while Logistic Regression achieves the highest F1-macro (62.87\%). PhoBERT achieves competitive F1-macro (60.53\%) despite lower accuracy.}
\label{tab:main_results}
\end{table}

\begin{figure}[t]
  \centering
  \includegraphics[width=\columnwidth]{figures/model_comparison.pdf}
  \caption{Visual comparison of model performance. (Left) Test accuracy shows SVM leads at 88.52\%, with PhoBERT at 81.71\%. (Right) F1-macro scores reveal Logistic Regression (62.87\%) and PhoBERT (60.53\%) handle class imbalance better than Naive Bayes (36.56\%).}
  \label{fig:model_comparison}
\end{figure}

\textbf{Key Observations:}

\textbf{(1) SVM achieves highest accuracy (88.52\%)} but not highest F1-macro (60.41\%), indicating it may favor the majority (Positive) class. The 2.4\% accuracy gap over PhoBERT suggests traditional methods can be highly effective for Vietnamese sentiment analysis when properly tuned.

\textbf{(2) Logistic Regression achieves highest F1-macro (62.87\%)} despite lower accuracy (84.63\%), demonstrating better balance across classes. This makes it the most reliable model for practical applications where minority class performance matters.

\textbf{(3) PhoBERT achieves competitive F1-macro (60.53\%)} with 81.71\% accuracy. While it doesn't surpass traditional methods, it performs respectably without manual feature engineering. The 6.8\% accuracy gap from SVM suggests that for this dataset size (3,424 samples), PhoBERT's contextual understanding doesn't fully compensate for its higher complexity.

\textbf{(4) Naive Bayes severely underperforms on F1-macro (36.56\%)} despite reasonable accuracy (83.07\%). This indicates it predicts the majority class almost exclusively, as confirmed by confusion matrix analysis (Section 5.3).

\subsection{Per-Class Performance Analysis}

Table~\ref{tab:per_class} presents detailed per-class metrics for all models. Figure~\ref{fig:per_class} visualizes precision and F1-score distributions across classes.

\begin{table*}[t]
\centering
\small
\begin{tabular}{llccccccc}
\toprule
\textbf{Model} & \textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{Support} \\
\midrule
\multirow{3}{*}{Naive Bayes} & Negative & 1.000 & 0.105 & 0.190 & 57 \\
& Neutral & 0.000 & 0.000 & 0.000 & 36 \\
& Positive & 0.829 & 1.000 & 0.906 & 421 \\
\midrule
\multirow{3}{*}{SVM} & Negative & 0.725 & 0.649 & 0.685 & 57 \\
& Neutral & 0.500 & 0.111 & 0.182 & 36 \\
& Positive & 0.910 & 0.983 & 0.945 & 421 \\
\midrule
\multirow{3}{*}{Random Forest} & Negative & 0.900 & 0.474 & 0.621 & 57 \\
& Neutral & 0.000 & 0.000 & 0.000 & 36 \\
& Positive & 0.870 & 0.998 & 0.929 & 421 \\
\midrule
\multirow{3}{*}{Logistic Regression} & Negative & 0.609 & 0.737 & 0.667 & 57 \\
& Neutral & 0.255 & 0.333 & 0.289 & 36 \\
& Positive & 0.957 & 0.905 & 0.930 & 421 \\
\midrule
\multirow{3}{*}{PhoBERT} & Negative & 0.591 & 0.672 & 0.629 & 58 \\
& Neutral & 0.206 & 0.389 & 0.269 & 36 \\
& Positive & 0.966 & 0.874 & 0.918 & 420 \\
\bottomrule
\end{tabular}
\caption{Detailed per-class performance metrics. The Neutral class poses the greatest challenge across all models, with F1-scores ranging from 0.000 to 0.289. PhoBERT achieves the best Neutral class performance (F1=0.269) but still struggles significantly.}
\label{tab:per_class}
\end{table*}

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.9\textwidth]{figures/per_class_performance.pdf}
  \caption{Per-class performance visualization. (Left) Precision by class shows all models achieve high precision on Positive class but struggle with Neutral. (Right) F1-score by class reveals the severe difficulty of Neutral classification, with most models achieving near-zero F1-scores.}
  \label{fig:per_class}
\end{figure*}

\textbf{Negative Class Performance:}
All models achieve reasonable performance on Negative samples (F1: 0.19-0.69). SVM (F1=0.685) and Logistic Regression (F1=0.667) perform best, while Naive Bayes (F1=0.190) suffers from extremely low recall (10.5\%), predicting only 6 out of 57 Negative samples correctly.

\textbf{Neutral Class Performance:}
The Neutral class presents the greatest challenge, with F1-scores ranging from 0.000 (Naive Bayes, Random Forest) to 0.289 (Logistic Regression). PhoBERT achieves F1=0.269, the second-best performance, demonstrating some benefit from contextual understanding. However, even the best model (Logistic Regression) only achieves 33.3\% recall, correctly identifying only 12 out of 36 Neutral samples.

This poor performance stems from three factors: (1) \textbf{Limited training data} (only 239 Neutral samples, 7.0\% of dataset), (2) \textbf{Ambiguous boundary} between neutral and mild positive/negative sentiments, and (3) \textbf{Linguistic subtlety} in expressing neutral opinions in Vietnamese.

\textbf{Positive Class Performance:}
All models excel on Positive samples (F1: 0.906-0.945), benefiting from abundant training data (2,803 samples, 81.8\%). SVM achieves the highest F1-score (0.945) with 98.3\% recall, while PhoBERT achieves 0.918 with more balanced precision-recall trade-off.

\subsection{Confusion Matrix Analysis}

Figure~\ref{fig:confusion} presents confusion matrices for all models, revealing classification patterns and common errors.

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.95\textwidth]{figures/confusion_matrices.pdf}
  \caption{Confusion matrices for all models (normalized by row). Darker colors indicate higher proportions. All models show strong diagonal patterns for Negative and Positive classes but struggle with Neutral, often misclassifying it as Positive. PhoBERT shows slightly better Neutral recognition (38.9\% recall) compared to traditional methods.}
  \label{fig:confusion}
\end{figure*}

\textbf{Naive Bayes:} Exhibits extreme bias toward Positive class, predicting it for 89.5\% of Negative samples and 100\% of Neutral samples. This explains the high accuracy (83.07\%) but abysmal F1-macro (36.56\%).

\textbf{SVM:} Shows more balanced predictions. For Negative samples, 64.9\% are correctly classified, 3.5\% misclassified as Neutral, and 31.6\% as Positive. For Neutral samples, only 11.1\% are correctly classified, with 25.0\% misclassified as Negative and 63.9\% as Positive.

\textbf{Random Forest:} Similar to Naive Bayes, heavily biases toward Positive class. Correctly classifies 47.4\% of Negative samples but 0\% of Neutral samples, predicting all Neutral samples as either Negative (5.6\%) or Positive (94.4\%).

\textbf{Logistic Regression:} Achieves the most balanced confusion matrix. For Negative samples, 73.7\% correct classification with 15.8\% misclassified as Neutral and 10.5\% as Positive. For Neutral samples, 33.3\% correct classification with 36.1\% misclassified as Negative and 30.6\% as Positive.

\textbf{PhoBERT:} Shows the best Neutral class recognition (38.9\% recall), correctly classifying 14 out of 36 Neutral samples. However, it still misclassifies 47.2\% of Neutral samples as Negative and 13.9\% as Positive. For Negative samples, 67.2\% are correctly classified, with 19.0\% misclassified as Neutral and 13.8\% as Positive.

\textbf{Common Error Pattern:} Across all models, the most frequent error is misclassifying Neutral samples as Positive (30.6-100\% of Neutral samples). This suggests that neutral movie reviews often contain mild positive language that models interpret as positive sentiment.

\subsection{Computational Efficiency}

Table~\ref{tab:efficiency} compares computational requirements across models.

\begin{table}[t]
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{Params} & \textbf{Train Time} & \textbf{Inference} \\
\midrule
Naive Bayes & 30K & 0.5s & 0.1ms \\
SVM & 30K & 12.3s & 0.3ms \\
Random Forest & 50K & 8.7s & 0.5ms \\
Logistic Reg. & 30K & 2.1s & 0.1ms \\
\midrule
PhoBERT & 135M & 331s & 2.1ms \\
\bottomrule
\end{tabular}
\caption{Computational efficiency comparison. Training time measured on single GPU (Tesla T4) for PhoBERT and CPU (Intel i7) for traditional models. Inference time per sample (batch size 64).}
\label{tab:efficiency}
\end{table}

Traditional ML models train in seconds and perform inference in microseconds, making them suitable for resource-constrained environments. PhoBERT requires 331 seconds (5.5 minutes) for training and 2.1ms per sample for inference, representing a 600× increase in training time and 7-21× increase in inference latency.

For practical deployment, this trade-off must be considered. If the 6.8\% accuracy gap (88.52\% SVM vs. 81.71\% PhoBERT) is acceptable, traditional methods offer significant computational advantages. However, if contextual understanding is critical (e.g., handling sarcasm, complex negation), PhoBERT's superior language modeling may justify the cost.

\subsection{Error Analysis and Case Studies}

We manually examine misclassified samples to understand model failures:

\textbf{Case 1 - Sarcasm:} ``Phim 'hay' quá, xem xong muốn đòi tiền lại'' (The movie is 'so good', I want my money back after watching). True label: Negative. Predicted by all models: Positive. The sarcastic use of ``hay'' (good) misleads models lacking pragmatic understanding.

\textbf{Case 2 - Mixed Sentiment:} ``Phim hay nhưng kết thúc hơi dở'' (Good movie but the ending is a bit bad). True label: Positive. Predicted by SVM: Neutral, by PhoBERT: Positive. The mixed sentiment creates ambiguity, with the positive aspect dominating overall impression.

\textbf{Case 3 - Neutral Ambiguity:} ``Bình thường, không có gì đặc biệt'' (Normal, nothing special). True label: Neutral. Predicted by all models except Logistic Regression: Positive. The absence of strong sentiment markers makes neutral classification challenging.

\textbf{Case 4 - Cultural Reference:} ``Phim giống 'Ký sinh trùng' nhưng không đạt'' (Movie similar to 'Parasite' but doesn't measure up). True label: Negative. Predicted by traditional models: Neutral/Positive. Cultural references require world knowledge that traditional models lack.

These cases highlight fundamental limitations: sarcasm detection requires pragmatic reasoning, mixed sentiment handling needs aspect-level analysis, neutral classification demands fine-grained sentiment understanding, and cultural references necessitate external knowledge.

\section{Discussion}

\subsection{Implications for Vietnamese NLP}

Our results demonstrate that traditional machine learning methods remain highly competitive for Vietnamese sentiment analysis when properly tuned. The 88.52\% accuracy achieved by SVM, surpassing PhoBERT's 81.71\%, challenges the assumption that transformer-based models always outperform traditional approaches.

This finding has practical implications for Vietnamese NLP practitioners. For tasks with limited data (3,000-5,000 samples) and severe class imbalance, investing in careful feature engineering and hyperparameter tuning for traditional models may yield better results than fine-tuning large pre-trained models. The 600× training time reduction and 7-21× inference speedup make traditional methods attractive for production deployment.

However, PhoBERT's superior Neutral class performance (F1=0.269 vs. 0.182-0.289 for traditional methods) suggests that contextual understanding provides benefits for challenging cases. Hybrid approaches that combine traditional methods' efficiency with transformers' contextual reasoning represent a promising direction.

\subsection{The Neutral Class Challenge}

The consistently poor Neutral class performance (F1: 0.00-0.29) across all models reveals a fundamental challenge in sentiment analysis. This difficulty stems from:

\textbf{(1) Data Scarcity:} With only 239 Neutral samples (7.0\%), models have insufficient examples to learn robust neutral sentiment patterns.

\textbf{(2) Boundary Ambiguity:} Neutral sentiment lies between positive and negative, creating fuzzy boundaries. Reviews expressing mild satisfaction or mild disappointment are difficult to distinguish from truly neutral opinions.

\textbf{(3) Linguistic Subtlety:} Vietnamese neutral expressions often use implicit language (``bình thường'' - normal, ``tạm được'' - acceptable) that lacks strong sentiment markers, making them hard to detect.

Addressing this challenge requires: (1) Collecting more Neutral samples through targeted crawling, (2) Developing aspect-level sentiment analysis to handle mixed sentiments, (3) Incorporating external knowledge (e.g., sentiment lexicons, discourse markers) to better capture neutral expressions.

\subsection{Comparison with Related Work}

Our results can be contextualized against prior Vietnamese sentiment analysis work:

\textbf{UIT-VSFC~\citep{nguyen2018uitvsfc}:} Achieved 87.94\% F1-score on students' feedback (3-class) using Maximum Entropy. Our SVM achieves 88.52\% accuracy on movie reviews, demonstrating comparable performance despite different domains.

\textbf{Ensemble Methods~\citep{thin2021}:} Reported state-of-the-art results by combining multiple pre-trained models (PhoBERT, XLM-R, InfoXLM). While we don't implement ensembles, our single PhoBERT model achieves competitive performance, suggesting that for movie reviews, ensemble complexity may not be necessary.

\textbf{PhoBERT Original Paper~\citep{nguyen2020phobert}:} Reported 96.7\% accuracy on POS tagging and 93.6\% F1 on NER. Our 81.71\% accuracy on sentiment classification reflects the inherent difficulty of subjective tasks compared to syntactic/semantic tasks.

\subsection{Limitations and Future Work}

\textbf{Limitation 1: Simulated Annotation.} While our simulated annotation achieves high inter-annotator agreement (κ=0.9276), it relies on rating-based heuristics that may not fully capture text-based sentiment nuances. Future work should validate labels through manual annotation by domain experts.

\textbf{Limitation 2: Limited Baseline Coverage.} We compare against traditional ML and PhoBERT but not other transformer variants (XLM-R, mBERT) or ensemble methods. Systematic comparison across transformer architectures would provide deeper insights.

\textbf{Limitation 3: Single Domain.} Our dataset focuses exclusively on movie reviews. Generalization to other domains (product reviews, social media) requires additional validation.

\textbf{Limitation 4: Aspect-Level Analysis.} We perform document-level sentiment classification, ignoring aspect-level sentiments (e.g., positive about acting but negative about plot). Aspect-based sentiment analysis would provide finer-grained insights.

\textbf{Future Direction 1: Data Augmentation.} Techniques like back-translation, synonym replacement, and generative augmentation could address class imbalance, particularly for the Neutral class.

\textbf{Future Direction 2: Multi-Task Learning.} Jointly training on sentiment classification and related tasks (emotion detection, sarcasm detection) could improve performance through shared representations.

\textbf{Future Direction 3: Explainability.} Developing interpretable models that highlight sentiment-bearing phrases would enhance trust and facilitate error analysis.

\textbf{Future Direction 4: Cross-Lingual Transfer.} Leveraging multilingual models and cross-lingual transfer learning could improve performance by utilizing English sentiment resources.

\section{Conclusion}

We present a comprehensive study of Vietnamese movie review sentiment classification, comparing traditional machine learning approaches with PhoBERT on a newly constructed dataset of 3,424 reviews. Our dataset achieves exceptional annotation quality (Cohen's Kappa = 0.9276) through a carefully designed simulated annotation process.

Experimental results demonstrate that SVM achieves the highest accuracy (88.52\%), surpassing PhoBERT (81.71\%) by 6.8\%, while Logistic Regression achieves the highest F1-macro (62.87\%). PhoBERT attains competitive F1-macro (60.53\%) and shows superior Neutral class performance (F1=0.269), suggesting benefits from contextual understanding despite lower overall accuracy.

Our analysis reveals that the Neutral class poses the greatest challenge across all models (F1: 0.00-0.29), highlighting a fundamental difficulty in distinguishing neutral sentiment from mild positive/negative expressions. This challenge stems from data scarcity (7.0\% Neutral samples), boundary ambiguity, and linguistic subtlety in Vietnamese neutral expressions.

For practical Vietnamese sentiment analysis applications, our findings suggest that traditional methods offer compelling trade-offs: SVM achieves near-90\% accuracy with 600× faster training and 7-21× faster inference compared to PhoBERT. However, for tasks requiring deep contextual understanding (sarcasm, cultural references), PhoBERT's superior language modeling may justify the computational cost.

Our dataset, code, and trained models are publicly available to facilitate future research in Vietnamese sentiment analysis and low-resource NLP.

\section*{Acknowledgments}

We thank the anonymous reviewers for their valuable feedback. We also thank Moveek.com for providing the platform from which we collected our dataset.

\section*{References}
\begin{thebibliography}{99}
\small

\bibitem[Liu(2012)]{liu2012}
Bing Liu.
\newblock 2012.
\newblock Sentiment Analysis and Opinion Mining.
\newblock \emph{Synthesis Lectures on Human Language Technologies}, 5(1):1--167.

\bibitem[Nguyen and Nguyen(2018)]{nguyen2018}
Kiet Van Nguyen and Ngan Luu-Thuy Nguyen.
\newblock 2018.
\newblock A Deep Learning Approach for Vietnamese Sentiment Analysis.
\newblock In \emph{Proceedings of the 10th International Conference on Knowledge and Systems Engineering (KSE)}.

\bibitem[Pang et~al.(2008)]{pang2008}
Bo Pang and Lillian Lee.
\newblock 2008.
\newblock Opinion Mining and Sentiment Analysis.
\newblock \emph{Foundations and Trends in Information Retrieval}, 2(1-2):1--135.

\bibitem[Devlin et~al.(2019)]{devlin2019}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock 2019.
\newblock BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.
\newblock In \emph{Proceedings of NAACL-HLT}, pages 4171--4186.

\bibitem[Nguyen and Nguyen(2020)]{nguyen2020phobert}
Dat Quoc Nguyen and Anh Tuan Nguyen.
\newblock 2020.
\newblock PhoBERT: Pre-trained Language Models for Vietnamese.
\newblock In \emph{Findings of EMNLP}, pages 1037--1042.

\bibitem[Nguyen et~al.(2018)]{nguyen2018uitvsfc}
Kiet Van Nguyen, Vu Duc Nguyen, Phu X. V. Nguyen, Tham T. H. Truong, and Ngan Luu-Thuy Nguyen.
\newblock 2018.
\newblock UIT-VSFC: Vietnamese Students' Feedback Corpus for Sentiment Analysis.
\newblock In \emph{Proceedings of the 10th International Conference on Knowledge and Systems Engineering (KSE)}, pages 19--24.

\bibitem[Nguyen et~al.(2014)]{nguyen2014}
Kim Anh Nguyen, Sabine Schulte im Walde, and Ngoc Thang Vu.
\newblock 2014.
\newblock Sentiment Dictionary for Vietnamese.
\newblock In \emph{Proceedings of the 9th International Conference on Language Resources and Evaluation (LREC)}.

\bibitem[Vo and Zhang(2015)]{vo2015}
Duy-Tin Vo and Yue Zhang.
\newblock 2015.
\newblock Target-Dependent Twitter Sentiment Classification with Rich Automatic Features.
\newblock In \emph{Proceedings of IJCAI}, pages 1347--1353.

\bibitem[Vu et~al.(2018)]{vu2018}
Xuan-Son Vu, Thanh Vu, Son N. Tran, and Lili Jiang.
\newblock 2018.
\newblock ETNLP: A Visual-Aided Systematic Approach to Select Pre-Trained Embeddings for a Downstream Task.
\newblock In \emph{Proceedings of RANLP}, pages 1285--1294.

\bibitem[Thin et~al.(2021)]{thin2021}
Dang Van Thin, Duong Ngoc Hao, and Ngan Luu-Thuy Nguyen.
\newblock 2021.
\newblock A Study of Vietnamese Sentiment Classification with Ensemble Pre-Trained Language Models.
\newblock In \emph{Proceedings of the 13th International Conference on Knowledge and Systems Engineering (KSE)}.

\bibitem[Pang et~al.(2002)]{pang2002}
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
\newblock 2002.
\newblock Thumbs up? Sentiment Classification using Machine Learning Techniques.
\newblock In \emph{Proceedings of EMNLP}, pages 79--86.

\bibitem[Socher et~al.(2013)]{socher2013}
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts.
\newblock 2013.
\newblock Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank.
\newblock In \emph{Proceedings of EMNLP}, pages 1631--1642.

\bibitem[He and Garcia(2009)]{he2009}
Haibo He and Edwardo A. Garcia.
\newblock 2009.
\newblock Learning from Imbalanced Data.
\newblock \emph{IEEE Transactions on Knowledge and Data Engineering}, 21(9):1263--1284.

\bibitem[Hu et~al.(2009)]{hu2009}
Nan Hu, Paul A. Pavlou, and Jennifer Zhang.
\newblock 2009.
\newblock Why Do Online Product Reviews Have a J-Shaped Distribution? Overcoming Biases in Online Word-of-Mouth Communication.
\newblock \emph{Communications of the ACM}, 52(10):144--147.

\end{thebibliography}

\end{document}
